{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f7f914-3fe1-454e-a976-49536dacc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules classiques\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modules pour data-viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Options\n",
    "pd.set_option(\"display.min_rows\", 10)\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "pd.set_option(\"max_colwidth\", 1000)\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.rc('figure',figsize=(17,13))\n",
    "\n",
    "# EDA\n",
    "from wordcloud import *\n",
    "\n",
    "# Preprocessing \n",
    "import re, string\n",
    "from emoji import demojize\n",
    "\n",
    "# Modélisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Autres\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# Serialization\n",
    "import pickle\n",
    "\n",
    "# Distance text\n",
    "from Levenshtein import distance\n",
    "\n",
    "# Wordcloud\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, ImageColorGenerator, get_single_color_func\n",
    "import cv2\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatizer\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "import random\n",
    "import dataframe_image as dfi \n",
    "\n",
    "from utils import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c6121-2671-4e36-960b-009552c43e29",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f166e-8d85-490b-9674-ebfaae862eb3",
   "metadata": {},
   "source": [
    "[Tuto Snscrape](https://betterprogramming.pub/how-to-scrape-tweets-with-snscrape-90124ed006af) <br>\n",
    "[Tuto Snscrape 2](https://medium.com/dataseries/how-to-scrape-millions-of-tweets-using-snscrape-195ee3594721) <br>\n",
    "[Dates des émissions](https://fr.wikipedia.org/wiki/Koh-Lanta_:_La_L%C3%A9gende#D%C3%A9roulement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02f3b0-4e69-431c-b88b-dcaacfc21b78",
   "metadata": {},
   "source": [
    "Scraping effectué à la commande console suivante:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9d394-1b4c-4d2c-a407-5b4bddff09b6",
   "metadata": {},
   "source": [
    "```\n",
    "snscrape --jsonl --progress --max-results 100000 twitter-search \"kohlanta since:2021-08-03 until:2021-08-24\" > ./data/emission_0.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f1e8b2-b577-41fe-bccc-a509447c036c",
   "metadata": {},
   "source": [
    "Le premier jeu de données concerne les tweets avant la première émission du **24/08**. Les données récupérées sont du 03/08 au 24/08 (exclu). <br>\n",
    "\n",
    "Ensuite, pour chaque émission, les tweets seront du **jour de l'émission** au **jour de la prochaine émission** (exclu). <br>\n",
    "\n",
    "Il faudra alors:\n",
    "1. Supprimer les tweets du jour de l'émission avant **21:05:00**\n",
    "2. Ajouter les tweets du jour de la prochaine émission avant **21:05:00**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cb698-d814-4b8d-b7af-594b186f5e15",
   "metadata": {},
   "source": [
    "# Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267b5eee-ab65-4976-afdc-cac7a9c77e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_col = ['id', 'date', 'content', 'user',\n",
    "              'replyCount', 'retweetCount', 'likeCount', 'quoteCount',\n",
    "              'lang', 'sourceLabel', # lang pour filtrer ; sourceLabel pour stats\n",
    "              'media', # Pour récupérer les photos\n",
    "               # 'retweetedTweet', \n",
    "              'quotedTweet', 'inReplyToTweetId', # Savoir si c'est un retweet, une réponse, une quote...\n",
    "              'mentionedUsers', # Pour repérer les comptes mentionnés le +\n",
    "              'hashtags' # Pour voir la répartition \n",
    "              ]\n",
    "\n",
    "\n",
    "\n",
    "def clean_dataset(df: pd.DataFrame, col_to_keep : list, since=None, until=None, debug=True):\n",
    "    '''\n",
    "    Cette fonction\n",
    "    1. Conserve uniquement les colonnes utiles\n",
    "    2. Restreint les tweets à un intervalle défini par [@since, @until]\n",
    "    3. Supprime les tweets doublons, non français ou encore qui n'ont pas Kohlanta (sous forme de hashtag ou non) dans le contenu de leur tweet\n",
    "    4. Extrait des informations essentielles \n",
    "    '''   \n",
    "    \n",
    "    if debug: \n",
    "        print('---------- Infos sur le dataset -----------')\n",
    "        print(f'Taille initiale: {df.shape}')\n",
    "    \n",
    "    \n",
    "    # 1. Conserve uniquement les colonnes utiles\n",
    "    to_return = df[col_to_keep]\n",
    "    \n",
    "    # 2. Restreint les tweets à un intervalle défini par [@since, @until]\n",
    "    to_return.date = to_return.date.apply(lambda x: x.astimezone('Europe/Paris'))\n",
    "    tmp_shape = to_return.shape\n",
    "    \n",
    "    if since is not None:\n",
    "        to_return = to_return[to_return.date >= since]\n",
    "    if until is not None:\n",
    "        to_return = to_return[to_return.date <= until]\n",
    "        \n",
    "    if debug: print(f'{tmp_shape[0] - to_return.shape[0]} tweets ne faisant pas partie de l\\'intervalle de temps étudié supprimés.')\n",
    "        \n",
    "    # 3. Supprime les tweets doublons\n",
    "    tmp_shape = to_return.shape\n",
    "    to_return.drop(index=to_return[to_return.content.duplicated()].index, inplace=True)\n",
    "    if debug: print(f'{tmp_shape[0] - to_return.shape[0]} duplicats supprimés.')\n",
    "    \n",
    "    # 3. Supprime les tweets non français\n",
    "    tmp_shape = to_return.shape\n",
    "    to_return.drop(index=to_return[to_return.lang != 'fr'].index, inplace = True)\n",
    "    to_return.drop(labels='lang', axis='columns', inplace=True)\n",
    "    \n",
    "    if debug: print(f'{tmp_shape[0] - to_return.shape[0]} tweets non français supprimés.')\n",
    "    \n",
    "    # 3. Certains tweets ne parlent pas de Koh-Lanta mais mentionne quelqu'un qui possède cela dans son username\n",
    "    tmp_shape = to_return.shape\n",
    "    tweets_without_kohlanta_content = to_return[~(to_return.content.str.contains('Koh Lanta', case=False) | \\\n",
    "                                    to_return.content.str.contains('KohLanta', case=False) | \\\n",
    "                                    to_return.content.str.contains('Koh-Lanta', case=False) | \\\n",
    "                                    to_return.content.str.contains('Koh- Lanta', case=False) | \\\n",
    "                                    to_return.content.str.contains('Koh -Lanta', case=False))]\n",
    "\n",
    "    to_return.drop(index=tweets_without_kohlanta_content.index, inplace=True)\n",
    "    if debug: print(f'{tmp_shape[0] - to_return.shape[0]} tweets avec des usernames incorrects supprimés')\n",
    "    \n",
    "    # 4. Extrait des infos essentielles\n",
    "    # Récupération de l'username\n",
    "    user_name = to_return.user.apply(func = lambda x: x['username'])\n",
    "    \n",
    "    to_return.drop('user', axis='columns', inplace=True)\n",
    "    to_return['user_name'] = user_name\n",
    "    \n",
    "    # Récupération des comptes mentionnés\n",
    "    def users_mentioned(list_users):\n",
    "        usernames = []\n",
    "        for ind_user in range(0, len(list_users)):\n",
    "            usernames.append(list_users[ind_user]['username'])\n",
    "        return usernames\n",
    "        \n",
    "    tmp = to_return.mentionedUsers.fillna(value=np.nan)\n",
    "    list_mentioned_users = tmp.apply(lambda x: users_mentioned(x) if type(x) == list else np.nan)\n",
    "    \n",
    "    to_return.drop('mentionedUsers', axis='columns', inplace=True)\n",
    "    to_return['mentioned_users'] = list_mentioned_users\n",
    "    \n",
    "    # Le tweet est-il une réponse ?\n",
    "    isReply = to_return.inReplyToTweetId.apply(lambda x: False if np.isnan(x) else True)\n",
    "    \n",
    "    to_return.drop('inReplyToTweetId', axis='columns', inplace=True)\n",
    "    to_return['isReply'] = isReply\n",
    "    \n",
    "    # Le tweet est-il une quote ?\n",
    "    isQuote = to_return.quotedTweet.apply(lambda x: False if x is None else True)\n",
    "    \n",
    "    to_return.drop('quotedTweet', axis='columns', inplace=True)\n",
    "    to_return['isQuote'] = isQuote\n",
    "    \n",
    "    # Extraction de l'URL de la photo s'il en contient une\n",
    "    def get_img_media_tweets(content):\n",
    "        if not (content is None): # On vérifie que l'objet n'est pas nul avant d'accéder à son contenu\n",
    "            if content[0]['_type'] == 'snscrape.modules.twitter.Photo':\n",
    "                return content[0]['fullUrl']\n",
    "\n",
    "        return np.nan # Si objet nul ou pas une photo\n",
    "    \n",
    "    urlImg = to_return.media.apply(get_img_media_tweets)\n",
    "    \n",
    "    to_return.drop('media', axis='columns', inplace=True)\n",
    "    to_return['urlImg'] = urlImg \n",
    "    \n",
    "    \n",
    "    if debug:\n",
    "        print('')\n",
    "        print(f'Premier tweet: {to_return.date.min()}')\n",
    "        print(f'Dernier tweet: {to_return.date.max()}')\n",
    "        print(f'{isReply.sum()} tweets sont des réponses')\n",
    "        print(f'{isQuote.sum()} tweets sont des quotes')\n",
    "        print(f'{urlImg.notna().sum()} tweets contiennent des images')\n",
    "        print('')\n",
    "        print(f'{df.shape[0]-to_return.shape[0]} lignes supprimées au total')\n",
    "        print(f'{df.shape[1]-to_return.shape[1]} colonnes supprimées au total')\n",
    "        print(f'Nouvelle taille: {to_return.shape}')\n",
    "        print('\\n')\n",
    "    \n",
    "    return to_return.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0005c24-6925-4220-a4f1-238c9d34efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "compte_candidats = {\n",
    "  '@TeheiuraTEAHUI':'teheuira',\n",
    "  '@Laurentmaistret':'laurent',\n",
    "  '@NamadiaKohlanta':'namadia',\n",
    "  '@Blz_Freddy':'freddy',\n",
    "  '@candicekohlanta':'candice',\n",
    "  '@alixkohlanta':'alix',\n",
    "  '@AlexandraKoh':'alexandra',\n",
    "  '@loickohlanta':'loic',\n",
    "  '@claude_KohLanta':'claude',\n",
    "  '@karimakohlanta':'karima',\n",
    "  '@Coumba_kohLanta':'coumba',\n",
    "  '@MaximeKohLanta':'maxime',\n",
    "  '@clemencecastel1':'clemence',\n",
    "  '@jadekl7':'jade',\n",
    "  '@DenisBrogniart':'denis',\n",
    "}\n",
    "\n",
    "def nlp_pipeline(text):\n",
    "    '''\n",
    "    Cette pipeline nettoie nos tweets, on part du moins restrictif (suppression de combinaison particulière) au plus restrictif \n",
    "    La pipeline suivie est la suivante:\n",
    "    0. Remplace les noms des comptes candidats par leur prénom\n",
    "    1. Passe l'ensemble du texte en minuscule\n",
    "    2. Remplace les line feed et carriage return par des espaces\n",
    "    3. Supprime les URLs\n",
    "    4. Supprime les mentions\n",
    "    5. Supprime tous les hashtags comment par #koh (insensible à la casse)\n",
    "    6. Remplace les émoticons par leurs émotions correspondantes\n",
    "    7. Supprime la ponctuation et les nombres\n",
    "    \n",
    "    '''\n",
    "\n",
    "    for compte, nom in compte_candidats.items():\n",
    "        text = text.replace(compte, nom)\n",
    "\n",
    "    text = text.lower() # Passe l'ensemble du texte en minuscule\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '') # Remplace les line feed et carriage return par des espaces \n",
    "    text = re.sub(r'www\\.\\S+|https?\\S+', '', text) # Supprime les URLs http ou https ou juste www.\n",
    "    text = re.sub(r'@\\S+', '', text) # Supprime les mentions ( Elles peuvent contenir le nom d'un twittos qui est le même que celui d'un candidat !)\n",
    "\n",
    "    # text = re.sub(r'@\\S+', '', text) # Supprime les mentions\n",
    "    \n",
    "    text = re.sub(r'#koh\\S+', 'koh lanta', text, flags=re.IGNORECASE) # Supprime tous les hashtags commençant par #koh (insensible à la casse)\n",
    "    text = text.replace('#', '') # Supprime les #\n",
    "     \n",
    "#     text = demojize(text, language='fr') # Traite les emojis\n",
    "#     text = text.replace('_', ' ')\n",
    "\n",
    "    text = text.replace('é','e').replace('è', 'e').replace('ê', 'e').replace('ï','i') # Supprime les accents\n",
    "    \n",
    "#     for p in string.punctuation: # Supprime la ponctuation // text = re.sub(r'[!\"#\\$%&\\'\\(\\)\\*\\+,-\\.\\/:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~]', '', text)\n",
    "#         text = text.replace(p, '')\n",
    "#     text = re.sub(r'[0-9]', '', text) # Supprime les nombres  \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bfcf3-82de-4f2e-a6a6-6e5f7b9cb154",
   "metadata": {},
   "source": [
    "# Récupération et traitement des données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6d53cb7-e15c-4df5-bfad-bc7f7dadcea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "em0 = pd.read_json('./raw_data/emission_0.json', lines=True)\n",
    "em1 = pd.read_json('./raw_data/emission_1.json', lines=True)\n",
    "em2 = pd.read_json('./raw_data/emission_2.json', lines=True)\n",
    "em3 = pd.read_json('./raw_data/emission_3.json', lines=True)\n",
    "em4 = pd.read_json('./raw_data/emission_4.json', lines=True)\n",
    "em5 = pd.read_json('./raw_data/emission_5.json', lines=True)\n",
    "em6 = pd.read_json('./raw_data/emission_6.json', lines=True)\n",
    "em7 = pd.read_json('./raw_data/emission_7.json', lines=True)\n",
    "em8 = pd.read_json('./raw_data/emission_8.json', lines=True)\n",
    "em9 = pd.read_json('./raw_data/emission_9.json', lines=True)\n",
    "em10 = pd.read_json('./raw_data/emission_10.json', lines=True)\n",
    "em11 = pd.read_json('./raw_data/emission_11.json', lines=True)\n",
    "em12 = pd.read_json('./raw_data/emission_12.json', lines=True)\n",
    "em13 = pd.read_json('./raw_data/emission_13.json', lines=True)\n",
    "em14 = pd.read_json('./raw_data/emission_14.json', lines=True)\n",
    "em15 = pd.read_json('./raw_data/emission_15.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d54d178c-b211-475c-a744-de160686ef30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# Avant la première émission\n",
    "until = pd.Timestamp('2021-08-24 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em0, em1], ignore_index=True)\n",
    "\n",
    "df_em0 = clean_dataset(df=tmp, col_to_keep=useful_col, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 1\n",
    "since = pd.Timestamp('2021-08-24 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-08-31 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em1, em2], ignore_index=True)\n",
    "\n",
    "df_em1 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 2\n",
    "since = pd.Timestamp('2021-08-31 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-09-14 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em2, em3], ignore_index=True)\n",
    "\n",
    "df_em2 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "\n",
    "# Emission 3\n",
    "since = pd.Timestamp('2021-09-14 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-09-21 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em3, em4], ignore_index=True)\n",
    "\n",
    "df_em3 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since,until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 4\n",
    "since = pd.Timestamp('2021-09-21 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-09-28 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em4, em5], ignore_index=True)\n",
    "\n",
    "df_em4 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 5\n",
    "since = pd.Timestamp('2021-09-28 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-10-05 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em5, em6], ignore_index=True)\n",
    "\n",
    "df_em5 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 6\n",
    "since = pd.Timestamp('2021-10-05 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-10-12 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em6, em7], ignore_index=True)\n",
    "\n",
    "df_em6 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 6\n",
    "since = pd.Timestamp('2021-10-12 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-10-19 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em7, em8], ignore_index=True)\n",
    "\n",
    "df_em7 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 8\n",
    "since = pd.Timestamp('2021-10-19 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-10-26 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em8, em9], ignore_index=True)\n",
    "\n",
    "df_em8 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 9\n",
    "since = pd.Timestamp('2021-10-26 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-11-02 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em9, em10], ignore_index=True)\n",
    "\n",
    "df_em9 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 10\n",
    "since = pd.Timestamp('2021-11-02 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-11-09 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em10, em11], ignore_index=True)\n",
    "\n",
    "df_em10 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 11\n",
    "since = pd.Timestamp('2021-11-09 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-11-23 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em11, em12], ignore_index=True)\n",
    "\n",
    "df_em11 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 12\n",
    "since = pd.Timestamp('2021-11-23 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-11-30 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em12, em13], ignore_index=True)\n",
    "\n",
    "df_em12 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 13\n",
    "since = pd.Timestamp('2021-11-30 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-12-07 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em13, em14], ignore_index=True)\n",
    "\n",
    "df_em13 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 14\n",
    "since = pd.Timestamp('2021-12-07 21:05:00', tz='Europe/Paris')\n",
    "until = pd.Timestamp('2021-12-14 21:05:00', tz='Europe/Paris')\n",
    "tmp = pd.concat([em14, em15], ignore_index=True)\n",
    "\n",
    "df_em14 = clean_dataset(df=tmp, col_to_keep=useful_col, since=since, until=until, debug=DEBUG)\n",
    "\n",
    "# Emission 15\n",
    "since = pd.Timestamp('2021-12-14 21:05:00', tz='Europe/Paris')\n",
    "\n",
    "df_em15 = clean_dataset(df=em15, col_to_keep=useful_col, since=since, debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355979f9-35ac-42b6-9a7e-829c450a80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_em0, df_em1, df_em2, df_em3, df_em4, df_em5, df_em6, df_em7, df_em8, df_em9, df_em10, df_em11, df_em12, df_em13, df_em14, df_em15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c915119-44d6-4193-9a8a-acae1caa4d83",
   "metadata": {},
   "source": [
    "# Création des datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b71ec7-f6d0-4796-8d45-0a4ea759f78f",
   "metadata": {},
   "source": [
    "## NER (Détection des noms des candidats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c637e8-774c-4063-903c-43154a2eb3e4",
   "metadata": {},
   "source": [
    "Ici, je repère l'ensemble des entités de personnes dans les tweets pour pouvoir déduire les appelations données à l'ensemble des candidats. <br>\n",
    "Ensuite j'effectue une distance de similarité entre ces \"appelations\" et les noms réels des candidats. <br>\n",
    "Cela me permet d'assigner l'ensemble des appelations à un candidat donné. <br>\n",
    "\n",
    "Voir: https://www.baeldung.com/cs/string-similarity-edit-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c017d73-bfbb-4824-9f73-ed3d6840865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n",
    "\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed674d18-d735-46a1-958b-deb3950365cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trouve_surnoms(df:pd.DataFrame, nom_candidats: list):\n",
    "    \n",
    "    df.content = df.content.apply(nlp_pipeline) # Nettoie notre corpus\n",
    "       \n",
    "    # Par batch de 512, récupère l'ensemble des entités \"Personnes\" trouvées dans le contenu des tweets\n",
    "    persons = []\n",
    "    start = 0\n",
    "    for stop in range(0, df.shape[0], 512):\n",
    "        output = nlp(' '.join(df.iloc[start:stop].content.values)) # Concatène 512 tweets en un seul string\n",
    "        start = stop\n",
    "        for entity in output:\n",
    "            if entity['entity_group'] == 'PER':\n",
    "                persons.append(entity['word']) # Ajoute l'entité reconnue à notre liste\n",
    "      \n",
    "    # On nettoie ici les noms/surnoms obtenus\n",
    "    persons = list(map(lambda x: x.split(), persons)) # On éclate en un unique mots les entités contenant plusieurs mots (Ex: 'Claude le meilleur')\n",
    "    clean = [] # clean est notre liste avec tous les surnoms trouvés écrits 'proprement'\n",
    "    for nom in persons: # On passe notre liste en 1D\n",
    "        if type(nom)==list:\n",
    "            clean += nom\n",
    "        else:\n",
    "            clean.append(nom)\n",
    "            \n",
    "    clean = [nom for nom in clean if (len(nom) >= 3 and len(nom) < 13)] # On supprime les noms trop courts ou trop longs\n",
    "    clean = np.unique(clean)\n",
    "    \n",
    "    # On calcule la distance de levenshtein entre les surnoms trouvés et les noms réels des candidats.\n",
    "    # Pour ensuite classer dans un dict les trouvailles\n",
    "    surnoms_trouves = {}\n",
    "    for nom in nom_candidats:\n",
    "        res = [] # res contient l'ensemble des distances noms - surnoms\n",
    "        distance_max = round(len(nom)*0.33 - 1) # La distance de levenshtein maximum acceptée pour considérer cela comme une faute de frappe/Une autre appellation du candidat\n",
    "        \n",
    "        for entity in clean: # Calcul de la distance pour chaque entité trouvée\n",
    "            res.append(distance(entity, nom.lower()))\n",
    "        res = pd.Series(res)\n",
    "        \n",
    "        surnoms_trouves[nom] = clean[res[res <= distance_max].index] # Assigne les surnoms < à une certaine distance définie aux noms des candidats\n",
    "        surnoms_trouves[nom] = np.append(surnoms_trouves[nom], nom.lower()) # Ajoute le nom du candidat à cette liste\n",
    "        \n",
    "        if nom == 'Teheuira':\n",
    "            surnoms_trouves[nom] = np.append(surnoms_trouves[nom], 'tehe') # Exception pour teheuira où l'on connait un de ces surnoms populaire\n",
    "        \n",
    "        surnoms_trouves[nom] = np.unique(surnoms_trouves[nom]) # On supprime les doublons\n",
    "                \n",
    "    return surnoms_trouves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a34aaf2c-9a06-4f71-9c3d-4dfb98ab45a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Freddy': array(['freddy'], dtype='<U10'),\n",
       " 'Teheuira': array(['tehe', 'teheiura', 'teheuira', 'teheura', 'tehueira', 'tehuira',\n",
       "        'tehura'], dtype='<U10'),\n",
       " 'Patrick': array(['patrick'], dtype='<U10'),\n",
       " 'Alexandra': array(['alexandra'], dtype='<U10'),\n",
       " 'Cindy': array(['cindy'], dtype='<U10'),\n",
       " 'Claude': array(['claude'], dtype='<U10'),\n",
       " 'Karima': array(['karima'], dtype='<U10'),\n",
       " 'Jade': array(['jade'], dtype='<U10'),\n",
       " 'Maxime': array(['maxime'], dtype='<U10'),\n",
       " 'Laurent': array(['laurent'], dtype='<U10'),\n",
       " 'Candice': array(['candice'], dtype='<U10'),\n",
       " 'Sam': array(['sam'], dtype='<U10'),\n",
       " 'Clementine': array(['clementine'], dtype='<U10'),\n",
       " 'Ugo': array(['ugo'], dtype='<U10'),\n",
       " 'Namadia': array(['amadia', 'namadia', 'namandia'], dtype='<U10'),\n",
       " 'Christelle': array(['christelle', 'cristelle'], dtype='<U10'),\n",
       " 'Coumba': array(['coumba', 'coumva', 'cumba', 'koumba'], dtype='<U10'),\n",
       " 'Loic': array(['loic'], dtype='<U10'),\n",
       " 'Clemence': array(['clemence'], dtype='<U10'),\n",
       " 'Phil': array(['phil'], dtype='<U10'),\n",
       " 'Alix': array(['alix'], dtype='<U10'),\n",
       " 'Denis': array(['denis'], dtype='<U10')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trouve_surnoms(df=df_em5, nom_candidats=NOMS_CANDIDATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1e13ca-f10d-4337-ac5d-d29c2b1db6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_surnames(from_em=0, until_em=15, dfs=dfs, nom_candidats=nom_candidats):\n",
    "    '''\n",
    "    Sauvegarde tous les surnoms des candidats dans un pickle\n",
    "    '''\n",
    "    for num in range(from_em, until_em+1):\n",
    "        path_dict = f'./data_jugement/surnoms/surnoms_em{num}.pickle'\n",
    "        df = dfs[num] # Récupère le dataset de l'émission\n",
    "        \n",
    "        \n",
    "        d = trouve_surnoms(df=df, nom_candidats=nom_candidats)\n",
    "        with open(path_dict, 'wb') as f:\n",
    "            pickle.dump(d, f) # Sauvegarde l'ensemble des surnoms trouvés pour l'émission @num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb7bfa6c-91d7-45b9-83f7-8e57a3149e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_surnames(from_em=14, until_em=15, dfs=dfs, nom_candidats=NOMS_CANDIDATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd93c14-861c-4f0f-a633-9781af76340c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['freddy']\n",
      "['teheirua', 'tehehura', 'theura', 'tehura', 'teheuira', 'teheiura', 'tehe', 'theheuira', 'teheiru', 'teheura', 'tehiura', 'tehoura', 'tehuira', 'theuira', 'teheiuira', 'heuira', 'tehueira', 'teiheura']\n",
      "['patrik', 'patrick']\n",
      "['alexandra', 'lexandra']\n",
      "['cindy']\n",
      "['claude', 'claud']\n",
      "['karima', 'karim', 'karma']\n",
      "['jade']\n",
      "['maxime', 'maxim', 'maxine']\n",
      "['laurent', 'laurant']\n",
      "['candice']\n",
      "['sam']\n",
      "['clementine']\n",
      "['ugo']\n",
      "['namadia', 'namandia', 'amadia']\n",
      "['christelle', 'cristelle', 'christel']\n",
      "['coumva', 'comba', 'koumba', 'couba', 'cumba', 'coumba']\n",
      "['loic']\n",
      "['clemence', 'clement']\n",
      "['phil']\n",
      "['alix']\n",
      "['donis', 'dennis', 'denis']\n"
     ]
    }
   ],
   "source": [
    "from utils import surnoms_trouves\n",
    "\n",
    "for nom in NOMS_CANDIDATS:\n",
    "    all_results, result = surnoms_trouves(nom, 15)\n",
    "    print(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db746780-2e16-4f0d-a5e6-96fe38db736c",
   "metadata": {},
   "source": [
    "## Création de datasets par candidats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646bc45-872b-4df9-b7cd-0c92812844cd",
   "metadata": {},
   "source": [
    "La cellule ci-dessous nous créé l'ensemble des datasets pour chaque candidats et pour chaque émission. <br>\n",
    "Etapes: \n",
    "1. Récupère tous les surnoms trouvés pour chaque candidat (fichiers pickle créés dans save_all_surnames)\n",
    "2. Création du dataset du candidat en fonction des surnoms trouvés\n",
    "3. Sauvegarde des datasets localement pour une réutilisation ultérieure !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e5f63b-c0d2-4d7d-8349-e1268155e55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: Sauver sous un autre format que .csv (trop lent)\n",
    "# Pickle --> best (+ rapide ecriture / )\n",
    "# (voir https://towardsdatascience.com/stop-using-csvs-for-storage-here-are-the-top-5-alternatives-e3a7c9018de0)\n",
    "\n",
    "def save_datasets_candidates(dfs, nom_candidats, from_em=0, until_em=15):\n",
    "    for num in range(from_em, until_em+1): # On parcourt les datasets\n",
    "        for nom in nom_candidats: # On parcourt les candidats\n",
    "            path_to_save = f'./data_jugement/em{num}/'\n",
    "            path_dict = f'./data_jugement/surnoms/surnoms_em{num}.pickle'\n",
    "            end_file = f'{nom.lower()}.pickle'\n",
    "            \n",
    "            df = dfs[num] # Récupère le dataset de l'émission\n",
    "            df.content = df.content.apply(nlp_pipeline)\n",
    "            \n",
    "            all_results, result = surnoms_trouves(nom, until_em) # Récupère tous les surnoms trouvés pour un candidat\n",
    "            pattern = '|'.join(list(map(lambda x: '\\\\b'+x+'\\\\b', all_results))) # Pattern à respecter dans la recherche des surnoms\n",
    "            reg_pattern = re.compile(pattern)\n",
    "            \n",
    "            to_save = df[df.content.str.contains(reg_pattern, regex=True)] # Récupère tous les tweets mentionnant @nom\n",
    "            to_save.to_pickle(path_to_save + end_file) # Sauvegarde le nouveau dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55fc1e8b-10ba-4e95-b3bd-21e8f531fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1h20 avec .csv\n",
    "# 1min35 avec pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03306929-36ec-460c-8319-2846e9274a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_datasets_candidates(dfs, NOMS_CANDIDATS, 0, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koh-koh-venv",
   "language": "python",
   "name": "koh-koh-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
